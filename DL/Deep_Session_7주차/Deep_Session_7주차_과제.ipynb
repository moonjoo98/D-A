{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ba843c",
   "metadata": {},
   "source": [
    "## 다양한 토큰화, 임베딩 방법 사용하여 모델 성능 높이기\n",
    "해당 데이터셋은 sports, entertainment, medical, politics라는 4가지의 다른 영역의 텍스트 데이터입니다.<br>\n",
    "텍스트를 통해 각 영역을 맞추는 task를 수행하시오.\n",
    "\n",
    "다양한 전처리, tokenization, embedding 방법을 사용하여 X_train, y_train을 통해 모델링을 수행하고 X_test와 y_test를 사용하여 성능을 검증, 비교하시오.\n",
    "\n",
    "성능은 accuracy로 측정하시오.\n",
    "\n",
    "<font color='red'>**※ 단, 검증 이외에 X_test, y_test를 사용해서는 안됩니다.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283939d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5452d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021eb4f1",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e43a4",
   "metadata": {},
   "source": [
    "# huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df013ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'C:\\Users\\82103\\deep-세션\\text_classification_dataset.csv' at C:\\Users\\82103\\deep-세션",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_33732/3682250446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mINPUT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"text_classification_dataset.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlm-roberta-large-finetuned-conll03-english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexample_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m     \u001b[1;31m# Create a dataset builder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1664\u001b[1;33m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[0;32m   1665\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1488\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1490\u001b[1;33m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[0;32m   1491\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m     \u001b[1;31m# Try packaged\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_PACKAGED_DATASETS_MODULES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         return PackagedDatasetModuleFactory(\n\u001b[0m\u001b[0;32m   1151\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mget_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[0mget_patterns_locally\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         )\n\u001b[1;32m--> 800\u001b[1;33m         \u001b[0mdata_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFilesDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_local_or_remote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownnload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m         \u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_PACKAGED_DATASETS_MODULES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[0mbuilder_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"hash\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_files\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mfrom_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns_for_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             out[key] = (\n\u001b[1;32m--> 576\u001b[1;33m                 DataFilesList.from_local_or_remote(\n\u001b[0m\u001b[0;32m    577\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mfrom_local_or_remote\u001b[1;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[0;32m    542\u001b[0m     ) -> \"DataFilesList\":\n\u001b[0;32m    543\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m         \u001b[0mdata_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve_patterns_locally_or_by_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_extensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[0morigin_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_origin_metadata_locally_or_by_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36mresolve_patterns_locally_or_by_urls\u001b[1;34m(base_path, patterns, allowed_extensions)\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mdata_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_resolve_single_pattern_locally\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_extensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m                 \u001b[0mdata_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\data_files.py\u001b[0m in \u001b[0;36m_resolve_single_pattern_locally\u001b[1;34m(base_path, pattern, allowed_extensions)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to find 'C:\\Users\\82103\\deep-세션\\text_classification_dataset.csv' at C:\\Users\\82103\\deep-세션"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "#MODEL = \"klue/roberta-base\"\n",
    "INPUT = \"text_classification_dataset.csv\"\n",
    "MAX_LEN = 256\n",
    "dataset = load_dataset(\"csv\", data_files=INPUT,split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['text'],padding=True, max_length=MAX_LEN,truncation=True)\n",
    "    if 'type' in examples:\n",
    "        outputs[\"type\"] = examples[\"type\"]\n",
    "    return outputs\n",
    "\n",
    "dataset = dataset.map(example_fn, remove_columns=['text', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ef0a04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.train_test_split(0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a97e9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(dataset['train'])['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a4b1b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.DataFrame(dataset['test'])['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9780d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.DataFrame(dataset['train'])['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "62b04300",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=pd.DataFrame(dataset['test'])['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a578b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8e9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74b02e64",
   "metadata": {},
   "source": [
    "# 기존 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "428eaab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ACNI2012 @TheToka920 Never knew having 1 or 2...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MYCA Magical Moments:\\n\\nSeptember, 2011: Sham...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The current state of last year's @BBL finalist...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@HOLLYJISOO Why did you bring a cricket...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Babar Azam only Pakistani included in the ICC ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    type\n",
       "0  @ACNI2012 @TheToka920 Never knew having 1 or 2...  sports\n",
       "1  MYCA Magical Moments:\\n\\nSeptember, 2011: Sham...  sports\n",
       "2  The current state of last year's @BBL finalist...  sports\n",
       "3         @HOLLYJISOO Why did you bring a cricket...  sports\n",
       "4  Babar Azam only Pakistani included in the ICC ...  sports"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('text_classification_dataset.csv')\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c7015863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'entertainment', 'medical', 'politics'], dtype=object)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "21299c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus['text'], corpus['type'], stratify=corpus['type'], random_state=220510, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b557a",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "aab834dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocessing(data):\n",
    "    data = data.map(lambda x: x.lower()) # 소문자화\n",
    "    data.str.replace(pat=r'[^\\w]', repl=r' ', regex=True)\n",
    "    data.apply(lambda x : ''.join([k for k in x if k not in string.punctuation]))\n",
    "\n",
    "    data.apply(lambda x : ''.join([k for k in x if k not in string.digits]))\n",
    "    # 이후 전처리를 추가하여 함수를 만드시오\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c07122",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f4c2621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 기준 tokenization\n",
    "def sep_based_tok(sentences):\n",
    "    toks = sentences.map(lambda x: x.split())\n",
    "    return toks\n",
    "\n",
    "# 다른 tokenization 방법을 사용해보시오.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13994f08",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ae21422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "def le_emb(toks, token_cb):\n",
    "    le = {token:i for i, token in enumerate(token_cb)}\n",
    "    embs = toks.map(lambda x: [le.get(tok, 0) for tok in x])\n",
    "    return embs\n",
    "\n",
    "## 다른 embedding 방법을 사용해보시오.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "bc64ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectorization(sentences, tok_method, emb_method):\n",
    "    sentences = preprocessing(sentences) # preprocessing\n",
    "    toks = tok_method(sentences) # tokenization\n",
    "    token_cb = ['<unk>'] + list({word for sentence in toks for word in sentence}) # make vocabulary\n",
    "    embs = emb_method(toks, token_cb) # word embedding\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d5935d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sentence_vectorization(X_train, sep_based_tok, le_emb)\n",
    "X_test = sentence_vectorization(X_test, sep_based_tok, le_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775639fe",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1909147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장비 할당\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e00431f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩: 입력 벡터 길이 맞추기\n",
    "X_train = torch.FloatTensor(pad_sequences(X_train)).unsqueeze(2)\n",
    "X_test = torch.FloatTensor(pad_sequences(X_test)).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8de04916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 데이터 전처리\n",
    "idx2label = dict(enumerate(y_train.unique()))\n",
    "label2idx = {label:idx for idx, label in idx2label.items()}\n",
    "y_train = [label2idx[x] for x in y_train]\n",
    "y_test = [label2idx[x] for x in y_test]\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "db099284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, random_state=220510, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a61d8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "val_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dfd94875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClf(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=5, dropout=.3, bidirectional=True):\n",
    "        super(SentenceClf, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_size*2, 4)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 4)\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(x)\n",
    "        last_output = output[:,-1,:]\n",
    "        return self.fc(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f963f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optim, loss_fc):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(text)\n",
    "        loss = loss_fc(preds, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = preds.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "        num_samples += target.size(0)\n",
    "        \n",
    "    train_loss /= num_samples\n",
    "    train_acc = 100 * correct /num_samples\n",
    "    \n",
    "    print(f'Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "\n",
    "def evaluate(model, test_dataloader, loss_fc):\n",
    "    model.eval() # 모델을 평가상태로 지정\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad(): # 평가 과정에서 gradient 업데이트를 하지 않기 위해\n",
    "        for batch in test_dataloader:\n",
    "            text = batch[0].to(device)\n",
    "            target = batch[1].to(device)\n",
    "            output = model(text)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            prediction = output.max(1, keepdim=True)[1] # 벡터 값 내 최대값으로 예측\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            \n",
    "            num_samples += target.size(0)\n",
    "            \n",
    "    test_loss /= num_samples\n",
    "    test_accuracy = 100 * correct /num_samples\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4aef6d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_epochs(model, train_dataloader, val_dataloader, optim, loss_fc, n_epoch):\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f'-----Epoch : {epoch+1}-----')\n",
    "        train(model, train_dataloader, optim, loss_fc)\n",
    "        valid_loss, valid_acc = evaluate(model, val_dataloader, loss_fc)\n",
    "        print(f'Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7caff680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch : 1-----\n",
      "Train Loss : 0.04549 | Train Acc : 22.75\n",
      "Valid Loss : 0.04587 | Valid Acc : 23.12\n",
      "-----Epoch : 2-----\n",
      "Train Loss : 0.0453 | Train Acc : 24.36\n",
      "Valid Loss : 0.04567 | Valid Acc : 23.66\n",
      "-----Epoch : 3-----\n",
      "Train Loss : 0.04532 | Train Acc : 27.19\n",
      "Valid Loss : 0.04552 | Valid Acc : 24.19\n",
      "-----Epoch : 4-----\n",
      "Train Loss : 0.04515 | Train Acc : 24.23\n",
      "Valid Loss : 0.04534 | Valid Acc : 23.12\n",
      "-----Epoch : 5-----\n",
      "Train Loss : 0.04503 | Train Acc : 24.23\n",
      "Valid Loss : 0.04522 | Valid Acc : 23.12\n",
      "-----Epoch : 6-----\n",
      "Train Loss : 0.04509 | Train Acc : 24.09\n",
      "Valid Loss : 0.0451 | Valid Acc : 25.27\n",
      "-----Epoch : 7-----\n",
      "Train Loss : 0.04474 | Train Acc : 27.86\n",
      "Valid Loss : 0.04499 | Valid Acc : 24.19\n",
      "-----Epoch : 8-----\n",
      "Train Loss : 0.04491 | Train Acc : 25.03\n",
      "Valid Loss : 0.04491 | Valid Acc : 23.66\n",
      "-----Epoch : 9-----\n",
      "Train Loss : 0.04479 | Train Acc : 25.57\n",
      "Valid Loss : 0.04483 | Valid Acc : 28.49\n",
      "-----Epoch : 10-----\n",
      "Train Loss : 0.04476 | Train Acc : 24.63\n",
      "Valid Loss : 0.04478 | Valid Acc : 29.03\n",
      "-----Epoch : 11-----\n",
      "Train Loss : 0.04472 | Train Acc : 27.19\n",
      "Valid Loss : 0.04473 | Valid Acc : 30.65\n",
      "-----Epoch : 12-----\n",
      "Train Loss : 0.04474 | Train Acc : 26.11\n",
      "Valid Loss : 0.0447 | Valid Acc : 30.11\n",
      "-----Epoch : 13-----\n",
      "Train Loss : 0.04484 | Train Acc : 26.11\n",
      "Valid Loss : 0.04463 | Valid Acc : 32.26\n",
      "-----Epoch : 14-----\n",
      "Train Loss : 0.04483 | Train Acc : 26.11\n",
      "Valid Loss : 0.04461 | Valid Acc : 32.26\n",
      "-----Epoch : 15-----\n",
      "Train Loss : 0.04489 | Train Acc : 25.84\n",
      "Valid Loss : 0.04456 | Valid Acc : 33.33\n",
      "-----Epoch : 16-----\n",
      "Train Loss : 0.04474 | Train Acc : 28.8\n",
      "Valid Loss : 0.04453 | Valid Acc : 33.33\n",
      "-----Epoch : 17-----\n",
      "Train Loss : 0.04463 | Train Acc : 28.67\n",
      "Valid Loss : 0.04451 | Valid Acc : 33.33\n",
      "-----Epoch : 18-----\n",
      "Train Loss : 0.04473 | Train Acc : 27.73\n",
      "Valid Loss : 0.04448 | Valid Acc : 33.33\n",
      "-----Epoch : 19-----\n",
      "Train Loss : 0.0445 | Train Acc : 29.21\n",
      "Valid Loss : 0.04446 | Valid Acc : 33.33\n",
      "-----Epoch : 20-----\n",
      "Train Loss : 0.04455 | Train Acc : 28.8\n",
      "Valid Loss : 0.04444 | Valid Acc : 32.8\n",
      "-----Epoch : 21-----\n",
      "Train Loss : 0.04466 | Train Acc : 27.86\n",
      "Valid Loss : 0.04444 | Valid Acc : 32.8\n",
      "-----Epoch : 22-----\n",
      "Train Loss : 0.04471 | Train Acc : 26.51\n",
      "Valid Loss : 0.04442 | Valid Acc : 32.26\n",
      "-----Epoch : 23-----\n",
      "Train Loss : 0.04464 | Train Acc : 28.67\n",
      "Valid Loss : 0.04441 | Valid Acc : 32.8\n",
      "-----Epoch : 24-----\n",
      "Train Loss : 0.04477 | Train Acc : 28.26\n",
      "Valid Loss : 0.0444 | Valid Acc : 32.8\n",
      "-----Epoch : 25-----\n",
      "Train Loss : 0.04474 | Train Acc : 27.32\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 26-----\n",
      "Train Loss : 0.04455 | Train Acc : 27.59\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 27-----\n",
      "Train Loss : 0.04486 | Train Acc : 28.13\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 28-----\n",
      "Train Loss : 0.04478 | Train Acc : 27.46\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 29-----\n",
      "Train Loss : 0.04458 | Train Acc : 28.94\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 30-----\n",
      "Train Loss : 0.04448 | Train Acc : 29.61\n",
      "Valid Loss : 0.04438 | Valid Acc : 32.8\n",
      "-----Epoch : 31-----\n",
      "Train Loss : 0.04463 | Train Acc : 27.73\n",
      "Valid Loss : 0.04438 | Valid Acc : 32.8\n",
      "-----Epoch : 32-----\n",
      "Train Loss : 0.04457 | Train Acc : 29.88\n",
      "Valid Loss : 0.0444 | Valid Acc : 32.8\n",
      "-----Epoch : 33-----\n",
      "Train Loss : 0.04456 | Train Acc : 29.21\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 34-----\n",
      "Train Loss : 0.04469 | Train Acc : 27.59\n",
      "Valid Loss : 0.04438 | Valid Acc : 32.8\n",
      "-----Epoch : 35-----\n",
      "Train Loss : 0.04464 | Train Acc : 26.38\n",
      "Valid Loss : 0.0444 | Valid Acc : 32.8\n",
      "-----Epoch : 36-----\n",
      "Train Loss : 0.04451 | Train Acc : 30.28\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 37-----\n",
      "Train Loss : 0.04471 | Train Acc : 28.67\n",
      "Valid Loss : 0.04438 | Valid Acc : 32.8\n",
      "-----Epoch : 38-----\n",
      "Train Loss : 0.04472 | Train Acc : 25.71\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 39-----\n",
      "Train Loss : 0.04476 | Train Acc : 28.53\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 40-----\n",
      "Train Loss : 0.04462 | Train Acc : 27.46\n",
      "Valid Loss : 0.04438 | Valid Acc : 32.8\n",
      "-----Epoch : 41-----\n",
      "Train Loss : 0.04458 | Train Acc : 29.34\n",
      "Valid Loss : 0.04437 | Valid Acc : 32.8\n",
      "-----Epoch : 42-----\n",
      "Train Loss : 0.04453 | Train Acc : 28.13\n",
      "Valid Loss : 0.04436 | Valid Acc : 32.8\n",
      "-----Epoch : 43-----\n",
      "Train Loss : 0.04446 | Train Acc : 30.28\n",
      "Valid Loss : 0.04437 | Valid Acc : 32.8\n",
      "-----Epoch : 44-----\n",
      "Train Loss : 0.0447 | Train Acc : 25.84\n",
      "Valid Loss : 0.04437 | Valid Acc : 32.8\n",
      "-----Epoch : 45-----\n",
      "Train Loss : 0.04464 | Train Acc : 27.19\n",
      "Valid Loss : 0.04437 | Valid Acc : 32.8\n",
      "-----Epoch : 46-----\n",
      "Train Loss : 0.04472 | Train Acc : 27.86\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 47-----\n",
      "Train Loss : 0.04466 | Train Acc : 25.71\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 48-----\n",
      "Train Loss : 0.0447 | Train Acc : 27.05\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 49-----\n",
      "Train Loss : 0.04465 | Train Acc : 28.67\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n",
      "-----Epoch : 50-----\n",
      "Train Loss : 0.04463 | Train Acc : 27.99\n",
      "Valid Loss : 0.04439 | Valid Acc : 32.8\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(0)\n",
    "\n",
    "model = SentenceClf().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_epochs(model, train_dataloader, val_dataloader, optimizer, criterion, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b95431",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "394c0197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.04403 | Test Acc : 32.8\n"
     ]
    }
   ],
   "source": [
    "# huggingface roberta pretrained conl33 tokenizer 활용\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7661bf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.04515 | Test Acc : 24.73\n"
     ]
    }
   ],
   "source": [
    "# huggingface klue/roberta_tokenizer 활용\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fada5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.04464 | Test Acc : 28.49\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 기반 tokenization + Label Encoding\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8719398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.04462 | Test Acc : 27.96\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 기반 tokenization + Label Encoding 불용어처리\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4646fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
