{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4d36ed",
   "metadata": {},
   "source": [
    "# Deep Seeeion 8주차 과제\n",
    "\n",
    "tweets를 기반으로 sentiment를 예측하는 task를 수행하고자 합니다. \n",
    "\n",
    "1. 데이터셋에 맞는 전처리를 진행해주세요.\n",
    "2. 실습 파일을 참고하여 Sentiment_RNN, Sentiment_LSTM, Sentiment_GRU class를 각각 완성해주세요.\n",
    "3. 3가지 모델에서의 성능을 비교해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5452d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021eb4f1",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "428eaab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      textID                                               text  \\\n",
       "0           0  cb774db0d1                I`d have responded, if I were going   \n",
       "1           1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2           2  088c60f138                          my boss is bullying me...   \n",
       "3           3  9642c003ef                     what interview! leave me alone   \n",
       "4           4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('Tweets.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a22745d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#na값 제거 해줘야함\n",
    "tweets.dropna(how='any',axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e7ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7015863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21299c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['text'], tweets['sentiment'], stratify=tweets['sentiment'], random_state=220510, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fed23",
   "metadata": {},
   "source": [
    "# twitter roberta tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edbd62b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['textID', 'text', 'selected_text', 'sentiment'],\n",
       "    num_rows: 27481\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba17d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6085decd612914ef\n",
      "Reusing dataset csv (C:\\Users\\Quantec\\.cache\\huggingface\\datasets\\csv\\default-6085decd612914ef\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accfda090c7f4eaca5820b55837a996f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27480 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "#MODEL = \"klue/roberta-base\"\n",
    "INPUT = \"Tweets.csv\"\n",
    "MAX_LEN = 256\n",
    "dataset = load_dataset(\"csv\", data_files=INPUT,split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['text'],padding=True, max_length=MAX_LEN,truncation=True)\n",
    "    if 'sentiment' in examples:\n",
    "        outputs[\"sentiment\"] = examples[\"sentiment\"]\n",
    "    return outputs\n",
    "\n",
    "dataset = dataset.map(example_fn, remove_columns=['text', 'sentiment'])\n",
    "\n",
    "dataset = dataset.train_test_split(0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97515028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'textID', 'selected_text', 'sentiment', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 21984\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'textID', 'selected_text', 'sentiment', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5496\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9b49f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(dataset['train'])['input_ids']\n",
    "X_test=pd.DataFrame(dataset['test'])['input_ids']\n",
    "y_train=pd.DataFrame(dataset['train'])['sentiment']\n",
    "y_test=pd.DataFrame(dataset['test'])['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f054723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e635f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad2b557a",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc72b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aab834dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    data = data.map(lambda x: x.lower())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c07122",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c2621d",
   "metadata": {
    "id": "f4c2621d"
   },
   "outputs": [],
   "source": [
    "# 띄어쓰기 기준 tokenization\n",
    "def sep_based_tok(sentences):\n",
    "    toks = sentences.map(lambda x: x.split())\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13994f08",
   "metadata": {
    "id": "13994f08"
   },
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bA_a-dqlRJL",
   "metadata": {
    "id": "1bA_a-dqlRJL"
   },
   "outputs": [],
   "source": [
    "# label encoding\n",
    "def le_emb(toks, token_cb):\n",
    "    le = {token:i for i, token in enumerate(token_cb)}\n",
    "    embs = toks.map(lambda x: [le.get(tok, 0) for tok in x])\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc64ba48",
   "metadata": {
    "id": "bc64ba48"
   },
   "outputs": [],
   "source": [
    "def sentence_vectorization(sentences, tok_method, emb_method):\n",
    "    sentences = preprocessing(sentences) # preprocessing\n",
    "    toks = tok_method(sentences) # tokenization\n",
    "    token_cb = ['<unk>'] + list({word for sentence in toks for word in sentence}) # make vocabulary\n",
    "    embs = emb_method(toks, token_cb) # word embedding\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5935d03",
   "metadata": {
    "id": "d5935d03"
   },
   "outputs": [],
   "source": [
    "X_train = sentence_vectorization(X_train, sep_based_tok, le_emb)\n",
    "X_test = sentence_vectorization(X_test, sep_based_tok, le_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775639fe",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1909147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장비 할당\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e00431f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩: 입력 벡터 길이 맞추기\n",
    "X_train = torch.FloatTensor(pad_sequences(X_train)).unsqueeze(2)\n",
    "X_test = torch.FloatTensor(pad_sequences(X_test)).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8de04916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 데이터 전처리\n",
    "idx2label = dict(enumerate(y_train.unique()))\n",
    "label2idx = {label:idx for idx, label in idx2label.items()}\n",
    "y_train = [label2idx[x] for x in y_train]\n",
    "y_test = [label2idx[x] for x in y_test]\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db099284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, random_state=220510, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a61d8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "val_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef0189",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6163ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sentiment_RNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=3, dropout=.3, bidirectional=True):\n",
    "        super(Sentiment_RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_size*2, 4)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 4)\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.rnn(x)\n",
    "        last_output = output[:,-1,:]\n",
    "        return self.fc(last_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0221ed",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "58b49b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sentiment_LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=3, dropout=.3, bidirectional=True):\n",
    "        super(Sentiment_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_size*2, 4)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 4)\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.lstm(x)\n",
    "        last_output = output[:,-1,:]\n",
    "        return self.fc(last_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d28dd6",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3ff64a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sentiment_GRU(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=3, dropout=.3, bidirectional=True):\n",
    "        super(Sentiment_GRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_size*2, 4)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 4)\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(x)\n",
    "        last_output = output[:,-1,:]\n",
    "        return self.fc(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b9564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f963f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optim, loss_fc):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        text = batch[0].to(device)\n",
    "        target = batch[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(text)\n",
    "        loss = loss_fc(preds, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = preds.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "        num_samples += target.size(0)\n",
    "        \n",
    "    train_loss /= num_samples\n",
    "    train_acc = 100 * correct /num_samples\n",
    "    \n",
    "    print(f'Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "\n",
    "def evaluate(model, test_dataloader, loss_fc):\n",
    "    model.eval() # 모델을 평가상태로 지정\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad(): # 평가 과정에서 gradient 업데이트를 하지 않기 위해\n",
    "        for batch in test_dataloader:\n",
    "            text = batch[0].to(device)\n",
    "            target = batch[1].to(device)\n",
    "            output = model(text)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            prediction = output.max(1, keepdim=True)[1] # 벡터 값 내 최대값으로 예측\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            \n",
    "            num_samples += target.size(0)\n",
    "            \n",
    "    test_loss /= num_samples\n",
    "    test_accuracy = 100 * correct /num_samples\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4aef6d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_epochs(model, train_dataloader, val_dataloader, optim, loss_fc, n_epoch):\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f'-----Epoch : {epoch+1}-----')\n",
    "        train(model, train_dataloader, optim, loss_fc)\n",
    "        valid_loss, valid_acc = evaluate(model, val_dataloader, loss_fc)\n",
    "        print(f'Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346344f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7caff680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch : 1-----\n",
      "Train Loss : 0.04282 | Train Acc : 31.32\n",
      "Valid Loss : 0.04194 | Valid Acc : 32.18\n",
      "-----Epoch : 2-----\n",
      "Train Loss : 0.04009 | Train Acc : 38.47\n",
      "Valid Loss : 0.03805 | Valid Acc : 39.37\n",
      "-----Epoch : 3-----\n",
      "Train Loss : 0.03646 | Train Acc : 40.21\n",
      "Valid Loss : 0.03568 | Valid Acc : 39.37\n",
      "-----Epoch : 4-----\n",
      "Train Loss : 0.03518 | Train Acc : 40.22\n",
      "Valid Loss : 0.03503 | Valid Acc : 39.37\n",
      "-----Epoch : 5-----\n",
      "Train Loss : 0.03475 | Train Acc : 40.33\n",
      "Valid Loss : 0.03476 | Valid Acc : 39.37\n",
      "-----Epoch : 6-----\n",
      "Train Loss : 0.03453 | Train Acc : 40.37\n",
      "Valid Loss : 0.03461 | Valid Acc : 39.37\n",
      "-----Epoch : 7-----\n",
      "Train Loss : 0.03441 | Train Acc : 40.29\n",
      "Valid Loss : 0.03452 | Valid Acc : 39.46\n",
      "-----Epoch : 8-----\n",
      "Train Loss : 0.03433 | Train Acc : 40.39\n",
      "Valid Loss : 0.03445 | Valid Acc : 39.46\n",
      "-----Epoch : 9-----\n",
      "Train Loss : 0.03426 | Train Acc : 40.37\n",
      "Valid Loss : 0.0344 | Valid Acc : 39.46\n",
      "-----Epoch : 10-----\n",
      "Train Loss : 0.03422 | Train Acc : 40.39\n",
      "Valid Loss : 0.03437 | Valid Acc : 39.46\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(0)\n",
    "\n",
    "# 모델을 변경해서 사용해주세요.\n",
    "model = Sentiment_LSTM().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_epochs(model, train_dataloader, val_dataloader, optimizer, criterion, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af066626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b95431",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d2e6f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.03424 | Test Acc : 41.3\n"
     ]
    }
   ],
   "source": [
    "#GRU + twitter tokenizer\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "31f8bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.03425 | Test Acc : 41.3\n"
     ]
    }
   ],
   "source": [
    "#LSTM + tiwtter tokenizer\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "151f3621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.03418 | Test Acc : 41.3\n"
     ]
    }
   ],
   "source": [
    "#RNN + twitter tokenizer\n",
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b40bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
